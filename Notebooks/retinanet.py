# -*- coding: utf-8 -*-
"""RetinaNet

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HP1qAy4mYvCtIMZbSYVcZOous2lMA6hi
"""

import locale
def getpreferredencoding(do_setlocale = True):
    return "UTF-8"
locale.getpreferredencoding = getpreferredencoding

import os

os.environ['KAGGLE_USERNAME'] = 'username'
os.environ['KAGGLE_KEY'] = 'key'

!pip install kaggle

!kaggle datasets download emirkiv/fracture-detection-ekiha-final
!unzip fracture-detection-ekiha-final.zip > /dev/null

import os
import csv
from PIL import Image

# Base directory
base_dir = "/content/dataset_final/4_dataset_aug_mixed"

# Dataset splits
splits = ["train", "test", "valid"]

image_extensions = [".jpg", ".jpeg", ".png"]

class_name = "fracture"

def convert_yolo_to_retinanet(split):
    images_dir = os.path.join(base_dir, split, "images")
    labels_dir = os.path.join(base_dir, split, "labels")
    output_csv = os.path.join(base_dir, f"{split}_labels.csv")

    with open(output_csv, mode="w", newline="") as csv_file:
        writer = csv.writer(csv_file)

        for image_name in os.listdir(images_dir):
            if not any(image_name.lower().endswith(ext) for ext in image_extensions):
                continue

            image_path = os.path.join(images_dir, image_name)
            label_name = os.path.splitext(image_name)[0] + ".txt"
            label_path = os.path.join(labels_dir, label_name)

            if not os.path.exists(label_path):
                continue

            # GÃ¶rÃ¼ntÃ¼ boyutlarÄ±nÄ± al
            with Image.open(image_path) as img:
                width, height = img.size

            with open(label_path, "r") as f:
                lines = f.readlines()

                for line in lines:
                    parts = line.strip().split()
                    if len(parts) != 5:
                        continue

                    _, x_center, y_center, w, h = map(float, parts)

                    x_center *= width
                    y_center *= height
                    w *= width
                    h *= height

                    x1 = int(x_center - w / 2)
                    y1 = int(y_center - h / 2)
                    x2 = int(x_center + w / 2)
                    y2 = int(y_center + h / 2)

                    writer.writerow([image_name, x1, y1, x2, y2, class_name])

for split in splits:
    convert_yolo_to_retinanet(split)

print("DÃ¶nÃ¼ÅŸtÃ¼rme iÅŸlemi tamamlandÄ±.")

from torch.utils.data import Dataset
from PIL import Image
import pandas as pd
import os
import torch

class CSVDataset(Dataset):
    def __init__(self, csv_file, img_dir, transforms=None):
        self.annotations = pd.read_csv(csv_file, header=None)
        self.annotations.columns = ['image_id', 'x1', 'y1', 'x2', 'y2', 'class']
        self.img_dir = img_dir
        self.transforms = transforms

        # Benzersiz sÄ±nÄ±f isimlerini bul ve integer etiketlere map et
        unique_classes = self.annotations['class'].unique()
        self.label_map = {name: idx for idx, name in enumerate(unique_classes)}

        # Etiketleri int'e Ã§evir
        self.annotations['class'] = self.annotations['class'].map(self.label_map)

        self.image_ids = self.annotations['image_id'].unique()

    def __len__(self):
        return len(self.image_ids)

    def __getitem__(self, idx):
        image_id = self.image_ids[idx]
        records = self.annotations[self.annotations['image_id'] == image_id]

        img_path = os.path.join(self.img_dir, image_id)
        img = Image.open(img_path).convert("RGB")

        boxes = records[['x1', 'y1', 'x2', 'y2']].values
        boxes = torch.as_tensor(boxes, dtype=torch.float32)

        labels = torch.as_tensor(records['class'].values, dtype=torch.int64)
        image_id_tensor = torch.tensor([idx])

        target = {
            "boxes": boxes,
            "labels": labels,
            "image_id": image_id_tensor,
        }

        if self.transforms:
            img = self.transforms(img)

        return img, target

from torchvision.transforms import functional as F

class ToTensor:
    def __call__(self, image):
        return F.to_tensor(image)

transforms = ToTensor()

dataset = CSVDataset(
    csv_file='/content/dataset_final/4_dataset_aug_mixed/train_labels.csv',
    img_dir='/content/dataset_final/4_dataset_aug_mixed/train/images',
    transforms=transforms
)

data_loader = torch.utils.data.DataLoader(
    dataset, batch_size=4, shuffle=True, collate_fn=lambda x: tuple(zip(*x))
)

from torchvision.models.detection import retinanet_resnet50_fpn

device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
model = retinanet_resnet50_fpn(pretrained=False, num_classes=1)
model.to(device)
model.train()
optimizer = torch.optim.SGD([p for p in model.parameters() if p.requires_grad], lr=0.001, momentum=0.9, weight_decay=0.0005)

for epoch in range(100):
    for images, targets in data_loader:
        images = [img.to(device) for img in images]
        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

        loss_dict = model(images, targets)
        losses = sum(loss for loss in loss_dict.values())

        optimizer.zero_grad()
        losses.backward()
        optimizer.step()

    print(f"Epoch {epoch+1} Loss: {losses.item():.4f}")

torch.save(model.state_dict(), 'retinanet_weights.pth')

import os
import torch
import pandas as pd
from PIL import Image, ImageFile
import torchvision.transforms as T
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from torchvision.models.detection import retinanet_resnet50_fpn

# ---- Bozuk gÃ¶rÃ¼ntÃ¼leri yÃ¼klemeyi engelleme ----
ImageFile.LOAD_TRUNCATED_IMAGES = True

# ---- Ayarlar ----
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
NUM_CLASSES = 1
IMAGE_FOLDER = '/content/dataset_final/4_dataset_aug_mixed/test/images'
CSV_PATH = '/content/dataset_final/4_dataset_aug_mixed/test_labels.csv'
THRESHOLD = 0.5

# ---- Etiket isimleri (string) ve ID eÅŸlemesi ----
LABEL_MAP = {0: 'fracture'}
INVERSE_LABEL_MAP = {v: k for k, v in LABEL_MAP.items()}

# ---- Modeli yÃ¼kle ----
model = retinanet_resnet50_fpn(weights=None, num_classes=NUM_CLASSES)
model.load_state_dict(torch.load('retinanet_weights.pth', map_location=DEVICE))
model.to(DEVICE)
model.eval()

# ---- Etiketleri yÃ¼kle ----
annotations = pd.read_csv(CSV_PATH, header=None)
annotations.columns = ['image_id', 'x1', 'y1', 'x2', 'y2', 'class']

# CSV'deki image_id'yi uzantÄ±sÄ±z hale getir
annotations['image_id'] = annotations['image_id'].str.replace('.jpg', '').str.replace('.png', '')

# ---- Ground Truth KutularÄ± Al ----
def get_ground_truth_boxes_for_image(img_path):
    image_id = os.path.basename(img_path).split('.')[0]  # Dosya uzantÄ±sÄ±nÄ± kaldÄ±rÄ±yoruz
    rows = annotations[annotations['image_id'] == image_id]
    if rows.empty:
        return torch.zeros((0, 4))  # EÄŸer ground truth yoksa, boÅŸ tensor dÃ¶ndÃ¼r
    boxes = rows[['x1', 'y1', 'x2', 'y2']].values
    return torch.tensor(boxes, dtype=torch.float32)

# ---- GÃ¶rselleri al ----
test_images = [
    os.path.join(IMAGE_FOLDER, f)
    for f in os.listdir(IMAGE_FOLDER)
    if os.path.isfile(os.path.join(IMAGE_FOLDER, f)) and f.lower().endswith(('.jpg', '.png', '.jpeg'))
]

# ---- Tahmin ve Ground Truth Listeleri ----
predictions = []
ground_truths = []

for filename in test_images:
    image_path = os.path.join(IMAGE_FOLDER, filename)

    try:
        # GÃ¶rselleri yÃ¼klerken hatalÄ± dosyalarÄ± atla
        image = Image.open(image_path).convert("RGB")
    except OSError:
        print(f"Warning: {filename} is corrupted and cannot be opened.")
        continue  # EÄŸer bozuk gÃ¶rsel varsa bir sonraki gÃ¶rsele geÃ§

    image_tensor = T.ToTensor()(image).unsqueeze(0).to(DEVICE)

    with torch.no_grad():
        outputs = model(image_tensor)[0]

    # Tahminler
    pred_boxes = outputs['boxes'][outputs['scores'] > THRESHOLD].cpu()
    pred_scores = outputs['scores'][outputs['scores'] > THRESHOLD].cpu()
    pred_labels = outputs['labels'][outputs['scores'] > THRESHOLD].cpu()

    # Ground Truth
    gt_rows = annotations[annotations['image_id'] == filename.split('/')[-1].replace('.jpg', '').replace('.png', '')]
    gt_boxes = torch.tensor(gt_rows[['x1', 'y1', 'x2', 'y2']].values, dtype=torch.float32)
    gt_labels = torch.tensor([INVERSE_LABEL_MAP[str(cls)] for cls in gt_rows['class']], dtype=torch.int64)

    predictions.append({"boxes": pred_boxes, "scores": pred_scores})
    ground_truths.append({"boxes": gt_boxes})

    # ðŸŽ¨ Subplot: sol = ground truth, saÄŸ = prediction
    fig, axes = plt.subplots(1, 2, figsize=(16, 8))

    # Sol: Ground Truth
    axes[0].imshow(image)
    axes[0].set_title("Ground Truth")
    axes[0].axis('off')
    for box, label in zip(gt_boxes, gt_labels):
        x1, y1, x2, y2 = box
        rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=2, edgecolor='g', facecolor='none')
        axes[0].add_patch(rect)
        label_name = LABEL_MAP.get(label.item(), str(label.item()))
        axes[0].text(x1, y1 - 5, f"{label_name}", color='white', bbox=dict(facecolor='green', alpha=0.5))

    # SaÄŸ: Prediction
    axes[1].imshow(image)
    axes[1].set_title("Prediction")
    axes[1].axis('off')
    for box, label, score in zip(pred_boxes, pred_labels, pred_scores):
        x1, y1, x2, y2 = box
        rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=2, edgecolor='r', facecolor='none')
        axes[1].add_patch(rect)
        label_name = LABEL_MAP.get(label.item(), str(label.item()))
        axes[1].text(x1, y1 - 5, f"{label_name} ({score:.2f})", color='white', bbox=dict(facecolor='red', alpha=0.5))

    plt.suptitle(f"{filename} â€“ Ground Truth vs. Prediction")
    plt.tight_layout()
    plt.show()

import os
import torch
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import seaborn as sns
from PIL import Image
from torchvision.models.detection import retinanet_resnet50_fpn
from torchvision import transforms
from torchvision.ops import box_iou

# ---- Ayarlar ----
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
NUM_CLASSES = 1
SCORE_THRESHOLD = 0.95
IMAGE_FOLDER = '/content/dataset_final/4_dataset_aug_mixed/test/images'
CSV_PATH = '/content/dataset_final/4_dataset_aug_mixed/test_labels.csv'
WEIGHTS_PATH = 'retinanet_weights.pth'

# ---- Model YÃ¼kleme ----
model = retinanet_resnet50_fpn(weights=None, num_classes=NUM_CLASSES)
model.load_state_dict(torch.load('retinanet_weights.pth', map_location=DEVICE))
model.to(DEVICE)
model.eval()

# ---- DÃ¶nÃ¼ÅŸtÃ¼r ----
transform = transforms.ToTensor()

# ---- Etiketleri YÃ¼kle ----
annotations = pd.read_csv(CSV_PATH, header=None)
annotations.columns = ['image_id', 'x1', 'y1', 'x2', 'y2', 'class']

# ---- Ground Truth KutularÄ± Al ----
def get_ground_truth_boxes_for_image(img_path):
    image_id = os.path.basename(img_path)
    rows = annotations[annotations['image_id'] == image_id]
    if rows.empty:
        return torch.zeros((0, 4))
    boxes = rows[['x1', 'y1', 'x2', 'y2']].values
    return torch.tensor(boxes, dtype=torch.float32)

# ---- GÃ¶rselleri Al ----
test_images = [
    os.path.join(IMAGE_FOLDER, f)
    for f in os.listdir(IMAGE_FOLDER)
    if os.path.isfile(os.path.join(IMAGE_FOLDER, f)) and f.lower().endswith(('.jpg', '.png', '.jpeg'))
]

# ---- Tahmin ve Ground Truth Listeleri ----
predictions = []
ground_truths = []

for img_path in test_images:
    image = Image.open(img_path).convert("RGB")
    image_tensor = transform(image).unsqueeze(0).to(DEVICE)

    with torch.no_grad():
        output = model(image_tensor)[0]

    # Tahminler
    pred_boxes = output['boxes'][output['scores'] > SCORE_THRESHOLD].cpu()
    pred_scores = output['scores'][output['scores'] > SCORE_THRESHOLD].cpu()
    pred_labels = output['labels'][output['scores'] > SCORE_THRESHOLD].cpu()

    # Ground Truth
    gt_rows = annotations[annotations['image_id'] == img_path.split('/')[-1]]
    gt_boxes = torch.tensor(gt_rows[['x1', 'y1', 'x2', 'y2']].values, dtype=torch.float32)
    gt_labels = torch.tensor([INVERSE_LABEL_MAP[str(label)] for label in gt_rows['class'].values], dtype=torch.int64)

    predictions.append({"boxes": pred_boxes, "scores": pred_scores})
    ground_truths.append({"boxes": gt_boxes})

# ---- mAP@0.5 Hesaplama ve Confusion Matrix ----
def compute_map50(predictions, ground_truths, iou_threshold=0.5):
    all_tp = 0
    all_fp = 0
    all_fn = 0

    for pred, gt in zip(predictions, ground_truths):
        pred_boxes = pred['boxes']
        pred_scores = pred['scores']
        gt_boxes = gt['boxes']

        if len(pred_boxes) == 0:
            all_fn += len(gt_boxes)
            continue

        if len(gt_boxes) == 0:
            all_fp += len(pred_boxes)
            continue

        ious = box_iou(pred_boxes, gt_boxes)
        matched_gt = torch.zeros(len(gt_boxes))

        for i in range(len(pred_boxes)):
            max_iou, max_j = torch.max(ious[i], dim=0)
            if max_iou >= iou_threshold and matched_gt[max_j] == 0:
                all_tp += 1
                matched_gt[max_j] = 1
            else:
                all_fp += 1

        all_fn += int((matched_gt == 0).sum())

    precision = all_tp / (all_tp + all_fp + 1e-6)
    recall = all_tp / (all_tp + all_fn + 1e-6)
    f1 = 2 * precision * recall / (precision + recall + 1e-6)
    ap50 = precision

    return {
        "mAP@0.5": round(ap50, 4),
        "Precision": round(precision, 4),
        "Recall": round(recall, 4),
        "F1 Score": round(f1, 4),
        "TP": all_tp,
        "FP": all_fp,
        "FN": all_fn
    }

# ---- SonuÃ§larÄ± YazdÄ±r ve Confusion Matrix Ã‡iz ----
metrics = compute_map50(predictions, ground_truths)
print("Evaluation Metrics:")
for k, v in metrics.items():
    print(f"{k}: {v}")

# ---- "retinanet_results" KlasÃ¶rÃ¼nÃ¼ OluÅŸtur ----
output_dir = "retinanet_results"
os.makedirs(output_dir, exist_ok=True)  # KlasÃ¶r yoksa oluÅŸtur

# ---- Confusion Matrix'Ä± Kaydet ----
confusion_matrix_filename = os.path.join(output_dir, f"confusion_matrix_iou_{0.5}_conf_{SCORE_THRESHOLD}.png")

# Confusion Matrix'Ä± Ã§iz
plt.figure(figsize=(6, 6))
sns.heatmap(torch.tensor([[metrics['TP'], metrics['FP']], [metrics['FN'], 0]]).numpy(), annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted Positive', 'Predicted Negative'], yticklabels=['Actual Positive', 'Actual Negative'])
plt.title(f"Confusion Matrix (IoU >= 0.5, Conf >= {SCORE_THRESHOLD})")
plt.xlabel('Predicted')
plt.ylabel('Actual')

# GÃ¶rÃ¼ntÃ¼yÃ¼ kaydetmeden Ã¶nce Ã¶nce kaydet ve sonra gÃ¶ster
plt.savefig(confusion_matrix_filename)
#plt.close()  # GÃ¶rÃ¼ntÃ¼yÃ¼ kaydettikten sonra bellekten kaldÄ±r

# GÃ¶rÃ¼ntÃ¼yÃ¼ ekranda gÃ¶ster
plt.show()

print(f"Confusion matrix saved as {confusion_matrix_filename}")

# ---- Metrikleri Text DosyasÄ±na Kaydet ----
metrics_filename = os.path.join(output_dir, f"evaluation_metrics_iou_{0.5}_conf_{SCORE_THRESHOLD}.txt")  # Dinamik dosya adÄ±

# Dosyaya baÅŸlÄ±k yazalÄ±m (IoU ve Conf Threshold bilgisi)
with open(metrics_filename, "w") as f:
    f.write(f"Evaluation Metrics (IoU Threshold = 0.5, Confidence Threshold = {SCORE_THRESHOLD})\n")
    f.write("="*50 + "\n\n")  # BaÅŸlÄ±k iÃ§in bir ayraÃ§

    # Metrikleri yazdÄ±rma
    for k, v in metrics.items():
        f.write(f"{k}: {v}\n")

print(f"Evaluation metrics saved as {metrics_filename}")

import shutil

# Kaynak dosya yolu
source_model_path = "/content/retinanet_weights.pth"

# Hedef klasÃ¶r yolu
output_dir = "retinanet_results"

# Hedef dosya yolu
destination_model_path = os.path.join(output_dir, "retinanet_weights.pth")

# Model dosyasÄ±nÄ± kopyala
shutil.copy(source_model_path, destination_model_path)

print(f"Model saved to {destination_model_path}")

!zip -r /content/retinanet_results.zip /content/retinanet_results